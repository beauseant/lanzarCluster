#!/bin/bash
CORES=64
TIMEOUT=300
SVERSION=3.2.1
PYSPARK_CONFIG_FILE_OPT=""
MEMORY_MAX=0
export MESOS_MASTER="mesos://zk://10.0.12.77:2181,10.0.12.78:2181,10.0.12.51:2181,10.0.12.60:2181,10.0.12.75:2181,10.0.12.76:2181,10.0.12.18:2181/mesos"
MONGO_API=1
SPARK_NLP=1
PACKAGES_CMD=""
HIVE_DB_PATH="${HOME}/.hive/"
RUNNING_DIRECTORY="."
export EXTRA_SPARK_CLASSPATH=""
export GPU_MAX=0
NUM_MAX_AGENTS=10
CREDENTIALFILE=""
CONSTRAINTS=""
EXTRA_JARS=""
MEM_WORKER=0
IS_JUPYTER=0
SCRIPT=""
PARAMETERS=""
SCRIPT_NAME=$( basename "${0}" )
if [ "${SCRIPT_NAME}" == "jupyter-spark" ]
then
  IS_JUPYTER=1
fi

while getopts "jC:c:n:T:V:F:H:G:N:t:W:D:X:J:M:L:S:P:" flag
  do
    case $flag in
        j)
          IS_JUPYTER=1
          ;;
        C)
          CREDENTIALFILE="${OPTARG}"
        ;;
        c)
          CORES=$OPTARG
        ;;
        n)
          TASKNAME="$OPTARG"
        ;;
        T)
          TIMEOUT=${OPTARG}
        ;;
        V)
          SVERSION=${OPTARG}
        ;;
        F)
          PYSPARK_CONFIG_FILE_OPT="--config=${HOME}/.jupyter/${OPTARG}.py"
        ;;
        H)
          MESOS_MASTER="${OPTARG}"
        ;;
        t)
          MEMORY_MAX=${OPTARG}
        ;;
        G)
          GPU_MAX=${OPTARG}
          ;;
        N)
          NUM_MAX_AGENTS=${OPTARG}
          ;;
        W)
          MEM_WORKER=${OPTARG}
          ;;
        D)
          RUNNING_DIRECTORY="${OPTARG}"
          ;;
        X)
          CONSTRAINTS="${OPTARG}"
          ;;
        J)
          EXTRA_JARS="${OPTARG}"
          ;;
        S)
          if [ ${IS_JUPYTER} -eq 1 ]
          then
            print "Parameter incompatible with Jupyter Spark"
          else
            SCRIPT="${OPTARG}"
          fi
          ;;
        P)
          if [ ${IS_JUPYTER} -eq 1 ]
          then
            print "Parameter incompatible with Jupyter Spark"
          else
            PARAMETERS="${OPTARG}"
          fi
          ;;
        *)
          printf "Parametro desconocido\n"
          exit 255
          ;;
    esac
  done

if [ "${IS_JUPYTER}" == 1 ]
then
  export PYSPARK_PYTHON=python3
  export PYSPARK_DRIVER_PYTHON=jupyter
  export SPARK_PYTHON_OPTIONS="--conf spark.pyspark.python=/usr/bin/python3 --conf spark.pyspark.driver.python=/usr/bin/jupyter"
fi

if [ "${CREDENTIALFILE}X" != "X" ]
then
  SPARK_MESOS_CREDENTIALS=$( cat ${CREDENTIALFILE} )
fi

NUM_MAX_CORES=$(( ${NUM_MAX_AGENTS} * ${CORES} ))
if [ ${MEM_WORKER} -eq 0 ]
then
  MEM_WORKER=$(( 4 * ${CORES} ))
fi

if [ -d /opt/spark-${SVERSION}-bin-2.6.0 ]
then
  SPARK_HOME=/opt/spark-${SVERSION}-bin-2.6.0
  HADOOP_VERSION=2.6.0
elif [ -d /opt/spark-${SVERSION}-bin-2.7.3 ]
then
  SPARK_HOME=/opt/spark-${SVERSION}-bin-2.7.3
  HADOOP_VERSION=2.7.3
elif [ -d /opt/spark-${SVERSION}-bin-2.8.2 ]
then
  SPARK_HOME=/opt/spark-${SVERSION}-bin-2.8.2
  HADOOP_VERSION=2.8.2
elif [ -d /opt/spark-${SVERSION}-bin-2.8.3 ]
then
  SPARK_HOME=/opt/spark-${SVERSION}-bin-2.8.3
  HADOOP_VERSION=2.8.3
elif [ -d /opt/spark-${SVERSION}-bin-3.3.0 ]
then
  SPARK_HOME=/opt/spark-${SVERSION}-bin-3.3.0
  HADOOP_VERSION=3.3.0
elif [ -d /opt/spark-${SVERSION}-bin-3.3.1 ]
then
  SPARK_HOME=/opt/spark-${SVERSION}-bin-3.3.1
  HADOOP_VERSION=3.3.1
else
  printf "Imposible determinar version HADOOP\n"
  exit 255
fi

export SITE_NAME=$( uname -n )
export SPARK_VH=$( echo ${SVERSION} | cut -d . -f 1 )
export SPARK_VL=$( echo ${SVERSION} | cut -d . -f 2 )
export HIVE_DB_PATH_M=$( uname -n )
export HIVE_DB_PATH="${HIVE_DB_PATH}/${HIVE_DB_PATH_M}.$$"
export SPARK_DIST_CLASSPATH=$( /opt/hadoop/bin/hadoop classpath )

if [ ! -d "${HIVE_DB_PATH}" ]
then
  mkdir -p "${HIVE_DB_PATH}"
fi

export HIVE_DB_PATH="file://${HIVE_DB_PATH}"

if [ "${SPARK_VH}"  -lt 2 ]
then
  export PACKAGES_CMD="${PACKAGES_CMD} --packages com.databricks:spark-csv_2.11:1.2.0"
  export PYTHONHASHSEED=1
else
  export PYTHONHASHSEED=0
fi

if [ $MONGO_API -gt 0 ]
then
  export PACKAGES_CMD="${PACKAGES_CMD} --conf spark.mongodb.input.uri=${MONGO_INPUT}"
  export PACKAGES_CMD="${PACKAGES_CMD} --conf spark.mongodb.output.uri=${MONGO_OUTPUT}"
  if [ "$SPARK_VH" -ge 2 ]
  then
    export PACKAGES_CMD="${PACKAGES_CMD} --packages org.mongodb.spark:mongo-spark-connector_2.11:2.0.0"
  else 
    export PACKAGES_CMD="${PACKAGES_CMD} --packages org.mongodb.spark:mongo-spark-connector_2.11:1.1.0"
  fi
fi

if [ ${SPARK_NLP} -gt 0 ]
then
#  export PACKAGES_CMD="${PACKAGES_CMD} --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.1"
  export PACKAGES_CMD="${PACKAGES_CMD} --jars hdfs://DTSCHDFSCluster/usr/lib/spark-nlp-assembly-3.4.0.jar"
fi

if [ "X${EXTRA_JARS}" != "X" ]
then
  export PACKAGES_CMD="${PACKAGES_CMD} --jars ${EXTRA_JARS}"
fi

export MESOS_SECURITY=""
if [ "${SPARK_MESOS_SECRET}X" == "X" ]
then
  if [ "${SPARK_MESOS_CREDENTIALS}X" != "X" ]
  then
    export MESOS_SECURITY="--conf spark.mesos.principal=${USER}"
    if [ "${SPARK_MESOS_SECRET}X" == "X" ]
    then
      export MESOS_SECURITY="${MESOS_SECURITY} --conf spark.mesos.secret=${SPARK_MESOS_CREDENTIALS}"
    fi
  fi
else
  export MESOS_SECURITY="--conf spark.mesos.principal=${USER}"
fi


export SPARK_DRIVER_OPTS="--conf spark.driver.maxResultSize=${MEMORY_MAX} --conf spark.local.dir=/export/workdir/spark/tmp --driver-memory 20G --driver-cores 8 --driver-class-path /etc/hadoop/hdfs-site.xml:/etc/hadoop/core-site.xml:/etc/hadoop --conf spark.sql.warehouse.dir=${HIVE_DB_PATH} --conf spark.mesos.role=${USER}"

export SPARK_EXECUTOR_OPTS="--conf spark.shuffle.io.connectionTimeout=180000 --executor-memory ${MEM_WORKER}G --conf spark.executor.cores=${CORES}  --conf spark.cores.max=${NUM_MAX_CORES} --conf spark.shuffle.service.enabled=true --conf spark.executorEnv.PYTHONHASHSEED=${PYTHONHASHSEED}"

if [ "X${CONSTRAINTS}" != "X" ]
then
  export SPARK_EXECUTOR_OPTS="${SPARK_EXECUTOR_OPTS} --conf spark.mesos.constraints=${CONSTRAINTS}"
fi

if [ "${GPU_MAX}" -gt 0 ]
then
  export SPARK_EXECUTOR_OPTS="${SPARK_EXECUTOR_OPTS} --conf spark.mesos.gpus.max=${GPU_MAX} --conf spark.executor.resource.gpu.amount=1 --conf spark.task.resource.gpu.amount=1 --conf spark.executor.resource.gpu.discoveryScript=${HOME}/bin/getGpusResources.sh"
fi

export SPARK_PARAMETERS="${SPARK_PYTHON_OPTIONS} --conf=spark.eventLog.dir=/var/tmp --conf spark.executor.uri=/opt/spark-dist/spark-${SVERSION}-bin-${HADOOP_VERSION}.tgz ${PACKAGES_CMD} ${MESOS_SECURITY} ${SPARK_EXECUTOR_OPTS} ${SPARK_DRIVER_OPTS} --master ${MESOS_MASTER}"

export LOCAL_SPARK_JAVA_OPTS="--conf spark.shuffle.service.enabled=true --conf spark.rpc.askTimeout=${TIMEOUT} --conf spark.network.timeout=${TIMEOUT}"

if [ -f "${HOME}"/etc/ssl/"${SITE_NAME}-ipython-${USER}.pem" ] && [ -f "${HOME}"/etc/ssl/"${SITE_NAME}-ipython-${USER}.key" ]
then
  PYSPARK_SSL_OPTIONS="--certfile=\"${HOME}/etc/ssl/${SITE_NAME}-ipython-${USER}.pem\" --keyfile=\"${HOME}/etc/ssl/${SITE_NAME}-ipython-${USER}.key\""
else
  PYSPARK_SSL_OPTIONS=""
fi

if [ ${IS_JUPYTER} -eq 1 ]
then
  export PYSPARK_DRIVER_PYTHON_OPTS="lab ${PYSPARK_SSL_OPTIONS} ${PYSPARK_CONFIG_FILE_OPT} --ip='0.0.0.0' --no-browser --notebook-dir='${RUNNING_DIRECTORY}/'"
  export SPARKR_SUBMIT_ARGS="${SPARK_PARAMETERS} ${LOCAL_SPARK_JAVA_OPTS} sparkr-shell "

  "${SPARK_HOME}/bin/pyspark" --name "${TASKNAME}:${SITE_NAME}" ${SPARK_PARAMETERS} ${LOCAL_SPARK_JAVA_OPTS}
else
  "${SPARK_HOME}/bin/spark-submit" --name "${TASKNAME}:${SITE_NAME}" ${SPARK_PARAMETERS} ${LOCAL_SPARK_JAVA_OPTS} "${SCRIPT}" ${PARAMETERS}
fi
